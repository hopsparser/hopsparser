# Layer dimensions
mlp_input: 64
mlp_tag_hidden: 16
mlp_arc_hidden: 128
mlp_lab_hidden: 32
# Training hyperparameters
encoder_dropout: 0.
mlp_dropout: 0.1
batch_size: 8
epochs: 2
lr:
  base: 0.00003
  schedule:
    shape: linear
    warmup_steps: 100
max_gradient_norm: 1.0
lexers:
  - name: word_embeddings
    type: words
    embedding_size: 16
    word_dropout: 0.1
  - name: char_level_embeddings
    type: chars_rnn
    embedding_size: 4
    lstm_output_size: 8
  - name: fasttext
    type: fasttext
    source: fasttext_model.bin
  - name: roberta_minuscule
    type: bert
    model: "lgrobol/roberta-minuscule"
    layers: "*"
    subwords_reduction: "mean"
