# Layer dimensions
mlp_input: 64
mlp_tag_hidden: 16
mlp_arc_hidden: 128
mlp_lab_hidden: 32
# Training hyperparameters
encoder_dropout: 0.1
mlp_dropout: 0.1
batch_size: 32
epochs: 2
lr:
  base: 0.001
lexers:
  - name: word_embeddings
    type: words
    embedding_size: 8
    word_dropout: 0.1